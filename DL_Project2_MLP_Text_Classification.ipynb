{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5db00378",
    "outputId": "d304d9de-e3d3-4ad5-b337-c3bc6351b6f2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (2.7.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchmetrics in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/shijia_huang/.local/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: packaging in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (20.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from torchmetrics) (4.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from packaging->torchmetrics) (2.4.6)\n",
      "Requirement already satisfied: six in /Users/shijia_huang/opt/anaconda3/lib/python3.7/site-packages (from packaging->torchmetrics) (1.14.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BvrkNWR5tMhE",
    "outputId": "6b2d988d-5405-4084-9986-129adf42653f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
      "Requirement already satisfied: torchdata==0.7.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchtext) (2.1.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.0->torchtext) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzgMJwpmc0iC"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "_FILL_ = '_FILL_'\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHdrXN4d1M_2"
   },
   "source": [
    "Set up the optimization problem where we take a random y of data and want theta to converge to this y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OsAbUVezIwD",
    "outputId": "74d8da5b-6691-46bc-a6e1-115d8b5382ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
      "Epoch:0 Loss: 0.40996935963630676\n",
      "Epoch:1 Loss: 0.3999693691730499\n",
      "Epoch:2 Loss: 0.3899693787097931\n",
      "Epoch:3 Loss: 0.37996938824653625\n",
      "Epoch:4 Loss: 0.36996936798095703\n",
      "Epoch:5 Loss: 0.3599693775177002\n",
      "Epoch:6 Loss: 0.34996935725212097\n",
      "Epoch:7 Loss: 0.33996936678886414\n",
      "Epoch:8 Loss: 0.3299693763256073\n",
      "Epoch:9 Loss: 0.3199693560600281\n",
      "Epoch:10 Loss: 0.30996936559677124\n",
      "Epoch:11 Loss: 0.299969345331192\n",
      "Epoch:12 Loss: 0.2899693548679352\n",
      "Epoch:13 Loss: 0.27996936440467834\n",
      "Epoch:14 Loss: 0.2699693739414215\n",
      "Epoch:15 Loss: 0.2599693536758423\n",
      "Epoch:16 Loss: 0.24996936321258545\n",
      "Epoch:17 Loss: 0.23996935784816742\n",
      "Epoch:18 Loss: 0.2299693524837494\n",
      "Epoch:19 Loss: 0.21996936202049255\n",
      "Epoch:20 Loss: 0.20996935665607452\n",
      "Epoch:21 Loss: 0.1999693661928177\n",
      "Epoch:22 Loss: 0.18996937572956085\n",
      "Epoch:23 Loss: 0.17996938526630402\n",
      "Epoch:24 Loss: 0.16996940970420837\n",
      "Epoch:25 Loss: 0.15996941924095154\n",
      "Epoch:26 Loss: 0.1499694287776947\n",
      "Epoch:27 Loss: 0.13996943831443787\n",
      "Epoch:28 Loss: 0.12996946275234222\n",
      "Epoch:29 Loss: 0.11996947228908539\n",
      "Epoch:30 Loss: 0.10996947437524796\n",
      "Epoch:31 Loss: 0.09996949136257172\n",
      "Epoch:32 Loss: 0.08996950834989548\n",
      "Epoch:33 Loss: 0.07996951043605804\n",
      "Epoch:34 Loss: 0.0699695274233818\n",
      "Epoch:35 Loss: 0.059969544410705566\n",
      "Epoch:36 Loss: 0.04996956139802933\n",
      "Epoch:37 Loss: 0.039969559758901596\n",
      "Epoch:38 Loss: 0.029969578608870506\n",
      "Epoch:39 Loss: 0.019969593733549118\n",
      "Epoch:40 Loss: 0.00996959488838911\n",
      "Epoch:41 Loss: 3.040545379917603e-05\n",
      "Epoch:42 Loss: 0.009969599545001984\n",
      "Epoch:43 Loss: 3.040545379917603e-05\n",
      "Epoch:44 Loss: 0.009969599545001984\n",
      "Epoch:45 Loss: 3.040545379917603e-05\n",
      "Epoch:46 Loss: 0.009969599545001984\n",
      "Epoch:47 Loss: 3.040545379917603e-05\n",
      "Epoch:48 Loss: 0.009969599545001984\n",
      "Epoch:49 Loss: 3.040545379917603e-05\n",
      "Epoch:50 Loss: 0.009969599545001984\n",
      "Epoch:51 Loss: 3.040545379917603e-05\n",
      "Epoch:52 Loss: 0.009969599545001984\n",
      "Epoch:53 Loss: 3.040545379917603e-05\n",
      "Epoch:54 Loss: 0.009969599545001984\n",
      "Epoch:55 Loss: 3.040545379917603e-05\n",
      "Epoch:56 Loss: 0.009969599545001984\n",
      "Epoch:57 Loss: 3.040545379917603e-05\n",
      "Epoch:58 Loss: 0.009969599545001984\n",
      "Epoch:59 Loss: 3.040545379917603e-05\n",
      "Epoch:60 Loss: 0.009969599545001984\n",
      "Epoch:61 Loss: 3.040545379917603e-05\n",
      "Epoch:62 Loss: 0.009969599545001984\n",
      "Epoch:63 Loss: 3.040545379917603e-05\n",
      "Epoch:64 Loss: 0.009969599545001984\n",
      "Epoch:65 Loss: 3.040545379917603e-05\n",
      "Epoch:66 Loss: 0.009969599545001984\n",
      "Epoch:67 Loss: 3.040545379917603e-05\n",
      "Epoch:68 Loss: 0.009969599545001984\n",
      "Epoch:69 Loss: 3.040545379917603e-05\n",
      "Epoch:70 Loss: 0.009969599545001984\n",
      "Epoch:71 Loss: 3.040545379917603e-05\n",
      "Epoch:72 Loss: 0.009969599545001984\n",
      "Epoch:73 Loss: 3.040545379917603e-05\n",
      "Epoch:74 Loss: 0.009969599545001984\n",
      "Epoch:75 Loss: 3.040545379917603e-05\n",
      "Epoch:76 Loss: 0.009969599545001984\n",
      "Epoch:77 Loss: 3.040545379917603e-05\n",
      "Epoch:78 Loss: 0.009969599545001984\n",
      "Epoch:79 Loss: 3.040545379917603e-05\n",
      "Epoch:80 Loss: 0.009969599545001984\n",
      "Epoch:81 Loss: 3.040545379917603e-05\n",
      "Epoch:82 Loss: 0.009969599545001984\n",
      "Epoch:83 Loss: 3.040545379917603e-05\n",
      "Epoch:84 Loss: 0.009969599545001984\n",
      "Epoch:85 Loss: 3.040545379917603e-05\n",
      "Epoch:86 Loss: 0.009969599545001984\n",
      "Epoch:87 Loss: 3.040545379917603e-05\n",
      "Epoch:88 Loss: 0.009969599545001984\n",
      "Epoch:89 Loss: 3.040545379917603e-05\n",
      "Epoch:90 Loss: 0.009969599545001984\n",
      "Epoch:91 Loss: 3.040545379917603e-05\n",
      "Epoch:92 Loss: 0.009969599545001984\n",
      "Epoch:93 Loss: 3.040545379917603e-05\n",
      "Epoch:94 Loss: 0.009969599545001984\n",
      "Epoch:95 Loss: 3.040545379917603e-05\n",
      "Epoch:96 Loss: 0.009969599545001984\n",
      "Epoch:97 Loss: 3.040545379917603e-05\n",
      "Epoch:98 Loss: 0.009969599545001984\n",
      "Epoch:99 Loss: 3.040545379917603e-05\n",
      "tensor([[0.9000, 0.5000, 0.3000]])\n",
      "tensor([[0.8942, 0.4943, 0.2942]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch._functorch.vmap import lazy_load_decompositions\n",
    "# Short Question\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Define y to be a target of dimension (1, 3) without a gradient\n",
    "y = torch.tensor([[0.9, 0.5, 0.3]], requires_grad=False)\n",
    "\n",
    "# Define theta to be a random tensor of dimension (1, 3) which requires a gradient; we want theta to converge to y\n",
    "theta = torch.randn(1, 3, requires_grad=True)\n",
    "print(theta)\n",
    "\n",
    "# Define an SGD optimizer with learning rate 0.01 which acts on theta\n",
    "optimizer = torch.optim.SGD([theta], lr=0.01)\n",
    "\n",
    "# Fil in the code below using the optimizer above to get theta to converge to y\n",
    "for epoch in range(100):\n",
    "  # Zero out the gradients of l with respect to theta\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Define a loss manually which is ||theta-x||_{2}^{2}, the L2 loss across all components\n",
    "  loss = torch.norm(theta - y, p = 2)\n",
    "\n",
    "  print('Epoch:{} Loss: {}'.format(epoch, loss))\n",
    "\n",
    "  # Get teh gradients of l with respect to theta\n",
    "  loss.backward()\n",
    "\n",
    "  # Update theta\n",
    "  optimizer.step()\n",
    "\n",
    "# These should look very similar\n",
    "print(y)\n",
    "print(theta)\n",
    "with torch.no_grad():\n",
    "  # Check the y and theta have converged to almost the same thing\n",
    "  loss = torch.norm(theta - y, p = 2)\n",
    "  assert (loss.item() - 0.0)**2 <= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgA6DK8R3BUU"
   },
   "source": [
    "### Note that if we don't implement optimizer.zero_grad():\n",
    "### The gradient information would cumulate over time, and optimizer.zero_grad() would help us zero out the gradients such that for the next epoch, we wouldn't carry any unnecessary previous gradient information.\n",
    "### If we only call optimizer.zero_grad() every 3 batches, for every 2nd (and 3rd) batch, we were essentially including gradient information from previous 1 (and 2) batche(s), so we make larger steps than we were supposed to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5tQSyJ9cIOU"
   },
   "source": [
    "# Neural Text Classifier - Information\n",
    "\n",
    "We will build a basic Neural Text Classifier. \n",
    "\n",
    "\n",
    "The at a high level, the idea of this model goes as follows.\n",
    "- We are given a training set $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{N}$ where each $x^{(i)}$ is a sentence and $y^{(i)}$ is a class label.\n",
    "- First, we need to loop over $\\{x^{(i)}\\}_{i=1}^{N}$ and get the Vocabulary, the number of unique words we see.\n",
    "- Once we do this, we will express each word as a one-hot representation. To do this, we will use a mapping from a unique word to an integer. For example, \"the\" might get index 3 and if there are 10 words (in the entire Vocabulary) then \"the\" would have a vector representation $x_{the} = (0,0,1,0,0,0,0,0,0,0)$. There will be many words in this Vocabulary, over 13,000. For this example, each word is mapped to a unique integer.\n",
    "- We will feed batches of data to the model and each batch will be transformed into a tensor with words each word transformed to its integer index in VOCAB below.\n",
    "- For example, we might get [[\"the man walks\"], [\"this is a sentence\"]] -> [[\"the\", \"man\", \"walks\"], [\"this\", \"is\", \"a\", \"sentence\"]] -> [[1, 4, 5], [6, 7, 8, 15]]. It depends on what unique integer each word gets.\n",
    "- Different sentences have different numbers of tokens but all batches need to be the same dimension (this is how PyTorch works), so we need a padding token. So, for example, if the batch size is B = 2 and we given two sentences like [\"a b c\", \"a b c d e\"] then as a tensor this will become [[1, 2, 3, 0, 0], [1, 2, 3, 4, 5]] and notice that we padded the first example so that the tensor is of dimension (2, 5) with M = 5. In some sense, in each batch we need to figure out the maximum number of tokens for an instance and pad each instance to have the same length as this longest instance. To do the above, use the [collate function](https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders). The idea here is that the Dataloader takes in raw data and the collate function is applied to this data, returning formatting tensors we can use later on in the optimization. You'll fill this in, using the hints.  \n",
    "- After padding, we feed batches of data to the classifier, these are of dimension (B, M). For example, we have a batch size of 2 above and M = 5. This will depend on the batch but here the batch size is B.\n",
    "- Once we feed in (B, M) data to the network, we rewrite this as (B, M, vocab_size) by using a one-hot representation for each word.\n",
    "- Then, we do as it hints in the model's forward method. We first take an average agross all the M elements of each element of the batch to get a (B, vocab_size) tensor that represents each instance. We pass this tensor through linear layer and nonlinear layers as unusual. The model returns logits, without the Softmax applied. This is a multiclass classfication task.\n",
    "\n",
    "Finally, we optimize the network and check it's train and validation set accuracies. We'll use both direct methods and torchmetrics to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcb35e9c"
   },
   "source": [
    "### Information (if interested in more)\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
    "- collate function: https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n",
    "- embedding layer: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9140e3c"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9ace94f"
   },
   "outputs": [],
   "source": [
    "# This is the dataset we will use\n",
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "# We will just use CPU here, but if you have time try \"cuda\"\n",
    "DEVICE = \"cpu\"\n",
    "LR = 8.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MIN_FREQUENCY = 20\n",
    "# Padding valued used; if we have a tensor data x = [[1,2,3], [4, 5], [1,2,3,4,5]] this needs padding\n",
    "# As a tensor, this is t = [[1, 2, 3, 0, 0], [4, 5, 0, 0, 0], [1, 2, 3, 4, 5]]\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3zuWGeqcDsI"
   },
   "source": [
    "# Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16f471ac"
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer by using get_tokenizer; pass \"basic_english\"\n",
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1b61bba",
    "outputId": "8e0d11dd-12e4-4b2d-9269-2f277db2064a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'some', 'text', '.', '.', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_english_tokenizer(\"This is some text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68a50055"
   },
   "outputs": [],
   "source": [
    "# Save the tokenizer as a contant; this is needed later\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8620a436"
   },
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c84225a"
   },
   "outputs": [],
   "source": [
    "# Loop through all the (label, text) data and yield a tokenized version of text\n",
    "def yield_tokens(data_iter):\n",
    "    tokens = []\n",
    "    for _, text in data_iter:\n",
    "        tokens.append(TOKENIZER(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEChT6jDeLXF"
   },
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "affa3375"
   },
   "outputs": [],
   "source": [
    "# Use build_vocab_from_iterator to get the the vocabulary\n",
    "# This is essentially a dictionary going from a word to a unique integer\n",
    "# Make sure to specify the specials\n",
    "VOCAB = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter),\n",
    "    min_freq = MIN_FREQUENCY,\n",
    "    specials=('<pad>', '<unk>')\n",
    ")\n",
    "\n",
    "# Set the default index to 1\n",
    "# Otherwise, VOCAB['unknownbigword'] will raise an Exception\n",
    "# I.e. we want '<unk>' to be the unknown word\n",
    "VOCAB.set_default_index(VOCAB['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62336be9"
   },
   "outputs": [],
   "source": [
    "assert VOCAB['<unk>'] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "518918c0"
   },
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de48bde8",
    "outputId": "3fc44169-b2e2-4619-a255-3cdde71e3c4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 437, 0, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB['yoyooyoyoy'], VOCAB['house'], VOCAB['<pad>'], VOCAB['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24247d56",
    "outputId": "64456c16-2de2-4d32-d1ae-18eb89ea4016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13798\n"
     ]
    }
   ],
   "source": [
    "print(len(VOCAB))\n",
    "\n",
    "# stoi = VOCAB.get_stoi()\n",
    "# print(len(stoi))\n",
    "# # print(len(set(yield_tokens(train_iter))))\n",
    "# yield_tokens(train_iter)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "574cce1b",
    "outputId": "19464d2a-a77d-4afb-9dc8-96004c87f574"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[437, 437, 4548, 1]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB(TOKENIZER(\"House house houses ThisisnotaKNownWord\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df651a44"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94741f76"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab.vocab_factory import Vocab\n",
    "\n",
    "# Utility to transform text into a list of ints\n",
    "# This shoould go \"a b c\" -> [\"a\", \"b\", \"c\"] -> [1, 2, 3], for example\n",
    "def text_pipeline(x):\n",
    "    # Apply tokenizer to x\n",
    "    tokens = TOKENIZER(x)\n",
    "\n",
    "\n",
    "    # Return the Vocab at those tokens\n",
    "\n",
    "    l = []\n",
    "    for token in tokens:\n",
    "\n",
    "      l.append(VOCAB[token])\n",
    "\n",
    "    return l\n",
    "\n",
    "# Return a 0 starting version of x\n",
    "# If x = \"1\" this should return 0\n",
    "# If x = \"3\" this should return 2, Etc.\n",
    "def label_pipeline(x):\n",
    "    return int(x) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e903610"
   },
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95311731"
   },
   "outputs": [],
   "source": [
    "\n",
    "# For a batch of data that might not be a tensor, return the batch in ternsor version\n",
    "# batch is a length B lsit of tuples where each element is (label, text)\n",
    "# label is a raw string like \"1\" here; text is a sentence like \"this is about soccer\"\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (label, text) in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3} and append it to label list\n",
    "        label_list.append(label_pipeline(label))\n",
    "\n",
    "        # Return a list of ints\n",
    "        processed_text = torch.tensor(text_pipeline(text))\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "    # Make label_list into a tensor of dtype=torch.int64\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    # Pad the sequence\n",
    "    # For Exmaple: if we had 2 elements and [[1, 2], [1,2,3,4]] in the text_list then we want\n",
    "    # to have [[1, 2, 0, 0], [1, 2, 3, 4]] in text_list and text_list is a tensor\n",
    "    # Look up pad_sequence and make sure you specify batch_first=True and specify the padding_value=0\n",
    "    text_list = torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Return the data and put it on a GPU or CPU, as needed\n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5da047d"
   },
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5d2ae25e",
    "outputId": "ed624b5f-41eb-4d73-b69b-bffac16f0c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4 ...\n"
     ]
    }
   ],
   "source": [
    "# Get an iterator for the AG_NEWS dataset and get the train version\n",
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "\n",
    "# Use the above to get the number of class elements\n",
    "tup = [label for label, text in train_iter]\n",
    "num_class = len(set(tup))\n",
    "# What are the classes?\n",
    "print(f\"The number of classes is {num_class} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eca51b36"
   },
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ab8cb7c"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "# A very naive model used to classify text\n",
    "class OneHotTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_class):\n",
    "        super(OneHotTextClassificationModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_class = num_class\n",
    "\n",
    "        # Have this layer take in data of dimension vocab_size and return data of dimension 100\n",
    "        self.fc1 =  nn.Linear(vocab_size, 100, bias = False)\n",
    "\n",
    "        # We will not use this, but see below as we want to mimic this layer using one_hot and fc1\n",
    "        self.e = nn.Embedding(vocab_size, 100)\n",
    "\n",
    "        # Have this layer take in 100 and return data of dimension num_class\n",
    "        self.fc2 = nn.Linear(100, num_class, bias = False)\n",
    "        self.init_weights()\n",
    "\n",
    "        # See forward below; we do not use this but you can use this if you want to to check\n",
    "        self.use_embedding_layer = False\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of fc1 to the same exact data as what self.e has\n",
    "        # Initialize the bias to zero\n",
    "        self.fc1.weight.data = numpy.transpose(self.e.weight.data)\n",
    "\n",
    "        # Unitialize fc2 to uniform between -0.5 and 0.5\n",
    "        initrange = -0.5\n",
    "        self.fc2.weight.data.uniform_(initrange,-initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, K = x.shape\n",
    "        # x is of dimension (B, K), where K is the maximum number of tokens in an element of the batch\n",
    "        if not self.use_embedding_layer:\n",
    "          # Transform x to a tensor where each element is one-hot encoded\n",
    "          x = F.one_hot(x,num_classes = self.vocab_size).float() ### to float, otherwise error in later codes\n",
    "          assert(x.shape == (B, K, self.vocab_size))\n",
    "\n",
    "          # Pass x through fc1 to get the row in fc1 correspondng to the row x is\n",
    "          x = self.fc1(x)\n",
    "          assert(x.shape == (B, K, 100))\n",
    "        else:\n",
    "          # Note: the above two steps should be the same as doing the command below\n",
    "          x = self.e(x)\n",
    "          assert(x.shape == (B, K, 100))\n",
    "\n",
    "        # Take the mean of the embedings for all words in each sentence\n",
    "        x = x.mean(dim = 1)\n",
    "        assert(x.shape == (B, 100))\n",
    "\n",
    "        # Apply ReLU to x\n",
    "        x = F.relu(x)\n",
    "        assert(x.shape == (B, 100))\n",
    "\n",
    "        # Pass through fc2\n",
    "        x = self.fc2(x)\n",
    "        assert(x.shape == (B, self.num_class))\n",
    "\n",
    "        # Return the Logits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a43d569e",
    "outputId": "cb0ec626-f92a-4c2a-c49a-17dd248b5290"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7fc034e070>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8994ae19"
   },
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaaa82a2"
   },
   "outputs": [],
   "source": [
    "# Map the data to the right format\n",
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Split data into train and validation\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "# Set up different DataLoaders\n",
    "train_dataloader = DataLoader(split_train_, batch_size = BATCH_SIZE, shuffle = True, collate_fn= collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size = BATCH_SIZE, shuffle = True, collate_fn= collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn= collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a5af374"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72b5bb91"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d58cc1a9",
    "outputId": "590b4cc6-b0cf-458e-cba8-a3361d8531fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Loss: 0.009969599545001984\n",
      "Epoch:1 Loss: 0.009969599545001984\n",
      "Epoch:2 Loss: 0.009969599545001984\n",
      "Epoch:3 Loss: 0.009969599545001984\n",
      "Epoch:4 Loss: 0.009969599545001984\n",
      "Epoch:5 Loss: 0.009969599545001984\n",
      "Epoch:6 Loss: 0.009969599545001984\n",
      "Epoch:7 Loss: 0.009969599545001984\n",
      "Epoch:8 Loss: 0.009969599545001984\n",
      "Epoch:9 Loss: 0.009969599545001984\n",
      "Epoch:10 Loss: 0.009969599545001984\n",
      "Epoch:11 Loss: 0.009969599545001984\n",
      "Epoch:12 Loss: 0.009969599545001984\n",
      "Epoch:13 Loss: 0.009969599545001984\n",
      "Epoch:14 Loss: 0.009969599545001984\n",
      "Epoch:15 Loss: 0.009969599545001984\n",
      "Epoch:16 Loss: 0.009969599545001984\n",
      "Epoch:17 Loss: 0.009969599545001984\n",
      "Epoch:18 Loss: 0.009969599545001984\n",
      "Epoch:19 Loss: 0.009969599545001984\n",
      "Epoch:20 Loss: 0.009969599545001984\n",
      "Epoch:21 Loss: 0.009969599545001984\n",
      "Epoch:22 Loss: 0.009969599545001984\n",
      "Epoch:23 Loss: 0.009969599545001984\n",
      "Epoch:24 Loss: 0.009969599545001984\n",
      "Epoch:25 Loss: 0.009969599545001984\n",
      "Epoch:26 Loss: 0.009969599545001984\n",
      "Epoch:27 Loss: 0.009969599545001984\n",
      "Epoch:28 Loss: 0.009969599545001984\n",
      "Epoch:29 Loss: 0.009969599545001984\n",
      "Epoch:30 Loss: 0.009969599545001984\n",
      "Epoch:31 Loss: 0.009969599545001984\n",
      "Epoch:32 Loss: 0.009969599545001984\n",
      "Epoch:33 Loss: 0.009969599545001984\n",
      "Epoch:34 Loss: 0.009969599545001984\n",
      "Epoch:35 Loss: 0.009969599545001984\n",
      "Epoch:36 Loss: 0.009969599545001984\n",
      "Epoch:37 Loss: 0.009969599545001984\n",
      "Epoch:38 Loss: 0.009969599545001984\n",
      "Epoch:39 Loss: 0.009969599545001984\n",
      "Epoch:40 Loss: 0.009969599545001984\n",
      "Epoch:41 Loss: 0.009969599545001984\n",
      "Epoch:42 Loss: 0.009969599545001984\n",
      "Epoch:43 Loss: 0.009969599545001984\n",
      "Epoch:44 Loss: 0.009969599545001984\n",
      "Epoch:45 Loss: 0.009969599545001984\n",
      "Epoch:46 Loss: 0.009969599545001984\n",
      "Epoch:47 Loss: 0.009969599545001984\n",
      "Epoch:48 Loss: 0.009969599545001984\n",
      "Epoch:49 Loss: 0.009969599545001984\n",
      "Epoch:50 Loss: 0.009969599545001984\n",
      "Epoch:51 Loss: 0.009969599545001984\n",
      "Epoch:52 Loss: 0.009969599545001984\n",
      "Epoch:53 Loss: 0.009969599545001984\n",
      "Epoch:54 Loss: 0.009969599545001984\n",
      "Epoch:55 Loss: 0.009969599545001984\n",
      "Epoch:56 Loss: 0.009969599545001984\n",
      "Epoch:57 Loss: 0.009969599545001984\n",
      "Epoch:58 Loss: 0.009969599545001984\n",
      "Epoch:59 Loss: 0.009969599545001984\n",
      "Epoch:60 Loss: 0.009969599545001984\n",
      "Epoch:61 Loss: 0.009969599545001984\n",
      "Epoch:62 Loss: 0.009969599545001984\n",
      "Epoch:63 Loss: 0.009969599545001984\n",
      "Epoch:64 Loss: 0.009969599545001984\n",
      "Epoch:65 Loss: 0.009969599545001984\n",
      "Epoch:66 Loss: 0.009969599545001984\n",
      "Epoch:67 Loss: 0.009969599545001984\n",
      "Epoch:68 Loss: 0.009969599545001984\n",
      "Epoch:69 Loss: 0.009969599545001984\n",
      "Epoch:70 Loss: 0.009969599545001984\n",
      "Epoch:71 Loss: 0.009969599545001984\n",
      "Epoch:72 Loss: 0.009969599545001984\n",
      "Epoch:73 Loss: 0.009969599545001984\n",
      "Epoch:74 Loss: 0.009969599545001984\n",
      "Epoch:75 Loss: 0.009969599545001984\n",
      "Epoch:76 Loss: 0.009969599545001984\n",
      "Epoch:77 Loss: 0.009969599545001984\n",
      "Epoch:78 Loss: 0.009969599545001984\n",
      "Epoch:79 Loss: 0.009969599545001984\n",
      "Epoch:80 Loss: 0.009969599545001984\n",
      "Epoch:81 Loss: 0.009969599545001984\n",
      "Epoch:82 Loss: 0.009969599545001984\n",
      "Epoch:83 Loss: 0.009969599545001984\n",
      "Epoch:84 Loss: 0.009969599545001984\n",
      "Epoch:85 Loss: 0.009969599545001984\n",
      "Epoch:86 Loss: 0.009969599545001984\n",
      "Epoch:87 Loss: 0.009969599545001984\n",
      "Epoch:88 Loss: 0.009969599545001984\n",
      "Epoch:89 Loss: 0.009969599545001984\n",
      "Epoch:90 Loss: 0.009969599545001984\n",
      "Epoch:91 Loss: 0.009969599545001984\n",
      "Epoch:92 Loss: 0.009969599545001984\n",
      "Epoch:93 Loss: 0.009969599545001984\n",
      "Epoch:94 Loss: 0.009969599545001984\n",
      "Epoch:95 Loss: 0.009969599545001984\n",
      "Epoch:96 Loss: 0.009969599545001984\n",
      "Epoch:97 Loss: 0.009969599545001984\n",
      "Epoch:98 Loss: 0.009969599545001984\n",
      "Epoch:99 Loss: 0.009969599545001984\n",
      "tensor([[0.9000, 0.5000, 0.3000]])\n",
      "tensor([[0.8942, 0.4943, 0.2942]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    # Put the model in train mode; this does not matter right now\n",
    "    _FILL_\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0.0\n",
    "    log_interval = 200\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Get the predictions\n",
    "        predicted_label = model(text)\n",
    "\n",
    "        # Get the loss.\n",
    "        loss = loss_fn(input=predicted_label, target=label)\n",
    "\n",
    "        # The loss is computed by taking a mean, get the sum of the terms on the numerator\n",
    "        with torch.no_grad():\n",
    "          total_loss += loss.item() * len(label)\n",
    "\n",
    "        # Do back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to have max norm 0.1\n",
    "        # Look up torch.nn.utils.clip_grad_norm\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(),0.1)\n",
    "\n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the accuracy\n",
    "        # predicted_label is (B, num_class) so take the argmax over the right dimension to get the actual label\n",
    "        total_acc += (predicted_label.argmax(dim = 1) == label).sum().item()\n",
    "\n",
    "        # Update the total number of items\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f} \"\n",
    "                \"| loss {:8.3f}\".format(\n",
    "                    epoch, idx,\n",
    "                    len(dataloader),\n",
    "                    total_acc / total_count,\n",
    "                    total_loss / total_count\n",
    "                    )\n",
    "            )\n",
    "            total_acc, total_count, total_loss = 0, 0, 0.0\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "  # Zero out the gradients of l with respect to theta\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Define a loss manually which is ||theta-x||_{2}^{2}, the L2 loss across all components\n",
    "  loss = torch.norm(theta - y, p = 2)\n",
    "\n",
    "  print('Epoch:{} Loss: {}'.format(epoch, loss))\n",
    "\n",
    "  # Get teh gradients of l with respect to theta\n",
    "  loss.backward()\n",
    "\n",
    "  # Update theta\n",
    "  optimizer.step()\n",
    "\n",
    "# These should look very similar\n",
    "print(y)\n",
    "print(theta)\n",
    "with torch.no_grad():\n",
    "  # Check the y and theta have converged to almost the same thing\n",
    "  loss = torch.norm(theta - y, p = 2)\n",
    "  assert (loss.item() - 0.0)**2 <= 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85722617"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "def evaluate(dataloader, model):\n",
    "    # Put the model in eval model; this does not matter right now\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # Set this to Accuracy from torchmetrics; use multiclass and specify the number of labels\n",
    "    accuracy_fn = Accuracy(task = 'multiclass',num_classes=num_class)\n",
    "\n",
    "    total_acc = 0.0\n",
    "    total_count = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            # Get the predictions\n",
    "            predicted_label = model(text)\n",
    "            # Get the number of samples we have, the denominator of accuracy\n",
    "            total_count += label.size(0) ##\n",
    "\n",
    "            # Get the total number of times we have the correct predictions, use accuracy_fn\n",
    "            total_acc += accuracy_fn(predicted_label, label).item() * len(label)\n",
    "\n",
    "            # Use accuracy_fn from torchmetrics to check that the total number of correct predictions is the same as if you use argmax on predicted_label\n",
    "            assert (\n",
    "                accuracy_fn(predicted_label,label).item() * len(label) == (predicted_label.argmax(dim = 1) == label).sum().item()\n",
    "            )\n",
    "\n",
    "\n",
    "    accuracy = total_acc/total_count\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3LjZHTdrWW6"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "We should get an accuracy > 80% for the training set. This might take quite a bit of time to run since we use one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTEl16pIBkTe"
   },
   "outputs": [],
   "source": [
    "# Set up the loss function\n",
    "# Note that this should be a multiclass classification problem and you take in logits\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# Instantiate the model\n",
    "# Pass in the number of elements in VOCAB and num_class\n",
    "model = OneHotTextClassificationModel(len(VOCAB), num_class).to(DEVICE)\n",
    "\n",
    "# Instantiate the SGD optimizer with parameters LR\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21ba24f3",
    "outputId": "7e486bd8-5cd7-45f2-f3e6-ff4ea07a5b01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-294ed9f31187>:30: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(),0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 7125 batches | accuracy    0.758 | loss    0.634\n",
      "| epoch   1 |   400/ 7125 batches | accuracy    0.762 | loss    0.625\n",
      "| epoch   1 |   600/ 7125 batches | accuracy    0.751 | loss    0.686\n",
      "| epoch   1 |   800/ 7125 batches | accuracy    0.773 | loss    0.653\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.774 | loss    0.613\n",
      "| epoch   1 |  1200/ 7125 batches | accuracy    0.767 | loss    0.627\n",
      "| epoch   1 |  1400/ 7125 batches | accuracy    0.771 | loss    0.631\n",
      "| epoch   1 |  1600/ 7125 batches | accuracy    0.773 | loss    0.612\n",
      "| epoch   1 |  1800/ 7125 batches | accuracy    0.761 | loss    0.635\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.757 | loss    0.643\n",
      "| epoch   1 |  2200/ 7125 batches | accuracy    0.784 | loss    0.584\n",
      "| epoch   1 |  2400/ 7125 batches | accuracy    0.789 | loss    0.570\n",
      "| epoch   1 |  2600/ 7125 batches | accuracy    0.767 | loss    0.600\n",
      "| epoch   1 |  2800/ 7125 batches | accuracy    0.779 | loss    0.597\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.765 | loss    0.626\n",
      "| epoch   1 |  3200/ 7125 batches | accuracy    0.783 | loss    0.598\n",
      "| epoch   1 |  3400/ 7125 batches | accuracy    0.776 | loss    0.608\n",
      "| epoch   1 |  3600/ 7125 batches | accuracy    0.808 | loss    0.545\n",
      "| epoch   1 |  3800/ 7125 batches | accuracy    0.783 | loss    0.589\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.792 | loss    0.569\n",
      "| epoch   1 |  4200/ 7125 batches | accuracy    0.787 | loss    0.589\n",
      "| epoch   1 |  4400/ 7125 batches | accuracy    0.782 | loss    0.583\n",
      "| epoch   1 |  4600/ 7125 batches | accuracy    0.782 | loss    0.593\n",
      "| epoch   1 |  4800/ 7125 batches | accuracy    0.778 | loss    0.602\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.783 | loss    0.590\n",
      "| epoch   1 |  5200/ 7125 batches | accuracy    0.795 | loss    0.563\n",
      "| epoch   1 |  5400/ 7125 batches | accuracy    0.798 | loss    0.555\n",
      "| epoch   1 |  5600/ 7125 batches | accuracy    0.794 | loss    0.575\n",
      "| epoch   1 |  5800/ 7125 batches | accuracy    0.787 | loss    0.580\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.800 | loss    0.560\n",
      "| epoch   1 |  6200/ 7125 batches | accuracy    0.808 | loss    0.553\n",
      "| epoch   1 |  6400/ 7125 batches | accuracy    0.793 | loss    0.578\n",
      "| epoch   1 |  6600/ 7125 batches | accuracy    0.808 | loss    0.529\n",
      "| epoch   1 |  6800/ 7125 batches | accuracy    0.788 | loss    0.579\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.794 | loss    0.571\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 359.59s | valid accuracy    0.832 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 7125 batches | accuracy    0.811 | loss    0.543\n",
      "| epoch   2 |   400/ 7125 batches | accuracy    0.793 | loss    0.570\n",
      "| epoch   2 |   600/ 7125 batches | accuracy    0.802 | loss    0.533\n",
      "| epoch   2 |   800/ 7125 batches | accuracy    0.794 | loss    0.575\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.794 | loss    0.570\n",
      "| epoch   2 |  1200/ 7125 batches | accuracy    0.792 | loss    0.589\n",
      "| epoch   2 |  1400/ 7125 batches | accuracy    0.818 | loss    0.546\n",
      "| epoch   2 |  1600/ 7125 batches | accuracy    0.806 | loss    0.547\n",
      "| epoch   2 |  1800/ 7125 batches | accuracy    0.803 | loss    0.580\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.805 | loss    0.545\n",
      "| epoch   2 |  2200/ 7125 batches | accuracy    0.818 | loss    0.523\n",
      "| epoch   2 |  2400/ 7125 batches | accuracy    0.812 | loss    0.531\n",
      "| epoch   2 |  2600/ 7125 batches | accuracy    0.790 | loss    0.578\n",
      "| epoch   2 |  2800/ 7125 batches | accuracy    0.811 | loss    0.519\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.796 | loss    0.569\n",
      "| epoch   2 |  3200/ 7125 batches | accuracy    0.815 | loss    0.513\n",
      "| epoch   2 |  3400/ 7125 batches | accuracy    0.819 | loss    0.558\n",
      "| epoch   2 |  3600/ 7125 batches | accuracy    0.812 | loss    0.533\n",
      "| epoch   2 |  3800/ 7125 batches | accuracy    0.803 | loss    0.557\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.822 | loss    0.500\n",
      "| epoch   2 |  4200/ 7125 batches | accuracy    0.806 | loss    0.559\n",
      "| epoch   2 |  4400/ 7125 batches | accuracy    0.812 | loss    0.536\n",
      "| epoch   2 |  4600/ 7125 batches | accuracy    0.821 | loss    0.492\n",
      "| epoch   2 |  4800/ 7125 batches | accuracy    0.798 | loss    0.558\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.822 | loss    0.518\n",
      "| epoch   2 |  5200/ 7125 batches | accuracy    0.813 | loss    0.507\n",
      "| epoch   2 |  5400/ 7125 batches | accuracy    0.822 | loss    0.505\n",
      "| epoch   2 |  5600/ 7125 batches | accuracy    0.805 | loss    0.529\n",
      "| epoch   2 |  5800/ 7125 batches | accuracy    0.812 | loss    0.534\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.816 | loss    0.518\n",
      "| epoch   2 |  6200/ 7125 batches | accuracy    0.827 | loss    0.491\n",
      "| epoch   2 |  6400/ 7125 batches | accuracy    0.810 | loss    0.518\n",
      "| epoch   2 |  6600/ 7125 batches | accuracy    0.809 | loss    0.545\n",
      "| epoch   2 |  6800/ 7125 batches | accuracy    0.812 | loss    0.526\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.823 | loss    0.485\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 360.33s | valid accuracy    0.820 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 7125 batches | accuracy    0.815 | loss    0.493\n",
      "| epoch   3 |   400/ 7125 batches | accuracy    0.822 | loss    0.502\n",
      "| epoch   3 |   600/ 7125 batches | accuracy    0.816 | loss    0.505\n",
      "| epoch   3 |   800/ 7125 batches | accuracy    0.818 | loss    0.502\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.831 | loss    0.472\n",
      "| epoch   3 |  1200/ 7125 batches | accuracy    0.813 | loss    0.521\n",
      "| epoch   3 |  1400/ 7125 batches | accuracy    0.803 | loss    0.544\n",
      "| epoch   3 |  1600/ 7125 batches | accuracy    0.816 | loss    0.488\n",
      "| epoch   3 |  1800/ 7125 batches | accuracy    0.807 | loss    0.521\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.826 | loss    0.495\n",
      "| epoch   3 |  2200/ 7125 batches | accuracy    0.823 | loss    0.488\n",
      "| epoch   3 |  2400/ 7125 batches | accuracy    0.823 | loss    0.500\n",
      "| epoch   3 |  2600/ 7125 batches | accuracy    0.836 | loss    0.477\n",
      "| epoch   3 |  2800/ 7125 batches | accuracy    0.818 | loss    0.514\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.828 | loss    0.479\n",
      "| epoch   3 |  3200/ 7125 batches | accuracy    0.827 | loss    0.489\n",
      "| epoch   3 |  3400/ 7125 batches | accuracy    0.830 | loss    0.492\n",
      "| epoch   3 |  3600/ 7125 batches | accuracy    0.825 | loss    0.499\n",
      "| epoch   3 |  3800/ 7125 batches | accuracy    0.833 | loss    0.447\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.822 | loss    0.504\n",
      "| epoch   3 |  4200/ 7125 batches | accuracy    0.838 | loss    0.453\n",
      "| epoch   3 |  4400/ 7125 batches | accuracy    0.816 | loss    0.521\n",
      "| epoch   3 |  4600/ 7125 batches | accuracy    0.813 | loss    0.512\n",
      "| epoch   3 |  4800/ 7125 batches | accuracy    0.824 | loss    0.493\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.832 | loss    0.469\n",
      "| epoch   3 |  5200/ 7125 batches | accuracy    0.830 | loss    0.475\n",
      "| epoch   3 |  5400/ 7125 batches | accuracy    0.822 | loss    0.492\n",
      "| epoch   3 |  5600/ 7125 batches | accuracy    0.817 | loss    0.493\n",
      "| epoch   3 |  5800/ 7125 batches | accuracy    0.822 | loss    0.497\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.835 | loss    0.456\n",
      "| epoch   3 |  6200/ 7125 batches | accuracy    0.811 | loss    0.522\n",
      "| epoch   3 |  6400/ 7125 batches | accuracy    0.815 | loss    0.502\n",
      "| epoch   3 |  6600/ 7125 batches | accuracy    0.829 | loss    0.471\n",
      "| epoch   3 |  6800/ 7125 batches | accuracy    0.828 | loss    0.495\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.820 | loss    0.503\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 360.40s | valid accuracy    0.815 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 7125 batches | accuracy    0.842 | loss    0.457\n",
      "| epoch   4 |   400/ 7125 batches | accuracy    0.833 | loss    0.479\n",
      "| epoch   4 |   600/ 7125 batches | accuracy    0.824 | loss    0.476\n",
      "| epoch   4 |   800/ 7125 batches | accuracy    0.827 | loss    0.487\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.835 | loss    0.466\n",
      "| epoch   4 |  1200/ 7125 batches | accuracy    0.841 | loss    0.461\n",
      "| epoch   4 |  1400/ 7125 batches | accuracy    0.828 | loss    0.482\n",
      "| epoch   4 |  1600/ 7125 batches | accuracy    0.835 | loss    0.473\n",
      "| epoch   4 |  1800/ 7125 batches | accuracy    0.833 | loss    0.472\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.826 | loss    0.483\n",
      "| epoch   4 |  2200/ 7125 batches | accuracy    0.828 | loss    0.489\n",
      "| epoch   4 |  2400/ 7125 batches | accuracy    0.834 | loss    0.475\n",
      "| epoch   4 |  2600/ 7125 batches | accuracy    0.843 | loss    0.464\n",
      "| epoch   4 |  2800/ 7125 batches | accuracy    0.841 | loss    0.457\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.837 | loss    0.453\n",
      "| epoch   4 |  3200/ 7125 batches | accuracy    0.847 | loss    0.445\n",
      "| epoch   4 |  3400/ 7125 batches | accuracy    0.840 | loss    0.451\n",
      "| epoch   4 |  3600/ 7125 batches | accuracy    0.839 | loss    0.435\n",
      "| epoch   4 |  3800/ 7125 batches | accuracy    0.832 | loss    0.466\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.842 | loss    0.441\n",
      "| epoch   4 |  4200/ 7125 batches | accuracy    0.823 | loss    0.491\n",
      "| epoch   4 |  4400/ 7125 batches | accuracy    0.838 | loss    0.472\n",
      "| epoch   4 |  4600/ 7125 batches | accuracy    0.844 | loss    0.437\n",
      "| epoch   4 |  4800/ 7125 batches | accuracy    0.834 | loss    0.471\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.833 | loss    0.462\n",
      "| epoch   4 |  5200/ 7125 batches | accuracy    0.833 | loss    0.480\n",
      "| epoch   4 |  5400/ 7125 batches | accuracy    0.827 | loss    0.485\n",
      "| epoch   4 |  5600/ 7125 batches | accuracy    0.835 | loss    0.485\n",
      "| epoch   4 |  5800/ 7125 batches | accuracy    0.844 | loss    0.445\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.837 | loss    0.491\n",
      "| epoch   4 |  6200/ 7125 batches | accuracy    0.834 | loss    0.459\n",
      "| epoch   4 |  6400/ 7125 batches | accuracy    0.837 | loss    0.452\n",
      "| epoch   4 |  6600/ 7125 batches | accuracy    0.820 | loss    0.502\n",
      "| epoch   4 |  6800/ 7125 batches | accuracy    0.828 | loss    0.484\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.841 | loss    0.439\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 360.57s | valid accuracy    0.851 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 7125 batches | accuracy    0.831 | loss    0.468\n",
      "| epoch   5 |   400/ 7125 batches | accuracy    0.848 | loss    0.423\n",
      "| epoch   5 |   600/ 7125 batches | accuracy    0.843 | loss    0.441\n",
      "| epoch   5 |   800/ 7125 batches | accuracy    0.838 | loss    0.452\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.843 | loss    0.447\n",
      "| epoch   5 |  1200/ 7125 batches | accuracy    0.830 | loss    0.468\n",
      "| epoch   5 |  1400/ 7125 batches | accuracy    0.838 | loss    0.428\n",
      "| epoch   5 |  1600/ 7125 batches | accuracy    0.843 | loss    0.452\n",
      "| epoch   5 |  1800/ 7125 batches | accuracy    0.840 | loss    0.444\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.841 | loss    0.444\n",
      "| epoch   5 |  2200/ 7125 batches | accuracy    0.836 | loss    0.463\n",
      "| epoch   5 |  2400/ 7125 batches | accuracy    0.855 | loss    0.434\n",
      "| epoch   5 |  2600/ 7125 batches | accuracy    0.833 | loss    0.467\n",
      "| epoch   5 |  2800/ 7125 batches | accuracy    0.863 | loss    0.411\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.839 | loss    0.438\n",
      "| epoch   5 |  3200/ 7125 batches | accuracy    0.844 | loss    0.439\n",
      "| epoch   5 |  3400/ 7125 batches | accuracy    0.825 | loss    0.483\n",
      "| epoch   5 |  3600/ 7125 batches | accuracy    0.841 | loss    0.445\n",
      "| epoch   5 |  3800/ 7125 batches | accuracy    0.842 | loss    0.458\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.836 | loss    0.459\n",
      "| epoch   5 |  4200/ 7125 batches | accuracy    0.840 | loss    0.445\n",
      "| epoch   5 |  4400/ 7125 batches | accuracy    0.849 | loss    0.433\n",
      "| epoch   5 |  4600/ 7125 batches | accuracy    0.836 | loss    0.449\n",
      "| epoch   5 |  4800/ 7125 batches | accuracy    0.840 | loss    0.474\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.845 | loss    0.445\n",
      "| epoch   5 |  5200/ 7125 batches | accuracy    0.835 | loss    0.472\n",
      "| epoch   5 |  5400/ 7125 batches | accuracy    0.848 | loss    0.431\n",
      "| epoch   5 |  5600/ 7125 batches | accuracy    0.829 | loss    0.483\n",
      "| epoch   5 |  5800/ 7125 batches | accuracy    0.842 | loss    0.460\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.838 | loss    0.450\n",
      "| epoch   5 |  6200/ 7125 batches | accuracy    0.847 | loss    0.430\n",
      "| epoch   5 |  6400/ 7125 batches | accuracy    0.844 | loss    0.430\n",
      "| epoch   5 |  6600/ 7125 batches | accuracy    0.844 | loss    0.436\n",
      "| epoch   5 |  6800/ 7125 batches | accuracy    0.837 | loss    0.448\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.844 | loss    0.433\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 360.44s | valid accuracy    0.879 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.867\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, loss_fn, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            accu_val\n",
    "            )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpxIvg3FZ6dj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
